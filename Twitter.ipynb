{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 of the Data Science challenge.\n",
    "#### Create a tool that visualises different metrics for different search results on Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy \n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bearer_token():\n",
    "    base_path = \"/home/adrian/test/Kahoot-challenge/keys/\"\n",
    "    with open(base_path + \"bearer_token.txt\") as f:\n",
    "        return f.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = get_bearer_token()\n",
    "search_count_url = \"https://api.twitter.com/2/tweets/counts/recent\"\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentTweetCountsPython,v2RecentSearchPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_count(query):\n",
    "    # Optional params: start_time,end_time,since_id,until_id,next_token,granularity\n",
    "    query_count_params = {'query': query,'granularity': 'minute'}\n",
    "    query_count_object = connect_to_endpoint(search_count_url, query_count_params)\n",
    "    \n",
    "    '''make dataframe out of query count numbers, and convert from string to timestamps'''\n",
    "    df_query_count = pd.DataFrame(query_count_object[\"data\"], columns=[\"start\",\"end\",\"tweet_count\"])\n",
    "    df_query_count.loc[:,\"start\"] = pd.to_datetime(df_query_count.loc[:,\"start\"],utc=True)\n",
    "    df_query_count.loc[:,\"end\"] = pd.to_datetime(df_query_count.loc[:,\"end\"],utc=True)\n",
    "     \n",
    "    '''Get the search counts for the query, 1 minute, 5 minutes, 15 minutes ago and total hits'''\n",
    "    time_steps = [1,5,15]\n",
    "    query_count = {}\n",
    "    timezone_delay = 120\n",
    "    for minutes_ago in time_steps:\n",
    "        time_delta = pd.to_datetime('today') - pd.Timedelta(120 + minutes_ago, 'minutes')\n",
    "        query_count[str(minutes_ago) + \" minutes ago\"] = df_query_count[df_query_count[\"start\"] >= time_delta.tz_localize('UTC')].tweet_count.sum()    \n",
    "    query_count[\"total\"] = query_count_object[\"meta\"][\"total_tweet_count\"]\n",
    "    return query_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(search_term):\n",
    "    '''\n",
    "    Due to limitations in results for each request from API, pagination needs to be implemented. Using next_token for this.\n",
    "    Will only iterate through the 1000 first results, as this should be more than enough to illustrate proof of concept\n",
    "    '''\n",
    "    \n",
    "    #hack to get RFC 3339 timestamp as required by twitter\n",
    "    start_time = pd.to_datetime('today') - pd.Timedelta(120 + 15, 'minutes')\n",
    "    start_time = start_time.isoformat(\"T\")+\"Z\"\n",
    "\n",
    "    # Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "    # expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "    query_search_params = {'query': search_term,'tweet.fields': 'author_id,created_at','max_results':'100', 'start_time':start_time}\n",
    "\n",
    "    tweets = []\n",
    "    for i in range(100,1001,100):\n",
    "        query_search_object = connect_to_endpoint(search_url, query_search_params)\n",
    "        tweets.extend(query_search_object[\"data\"])\n",
    "        try: \n",
    "            next_token = query_search_object[\"meta\"]['next_token']\n",
    "            query_search_params[\"next_token\"] = query_search_object[\"meta\"][\"next_token\"]\n",
    "        except:\n",
    "            #no more content, exit loop\n",
    "            break\n",
    "            \n",
    "    df_tweets = pd.DataFrame(tweets)\n",
    "    df_tweets[\"created_at\"] = pd.to_datetime(df_tweets.loc[:,\"created_at\"],utc=True)\n",
    "    return df_tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_words(query_word,corpus,n_words):\n",
    "    '''\n",
    "    Returns the most frequently used words from the input corpus. \n",
    "    Removes punctuation and english stop words. \n",
    "    Removing english stop words can be a weakness in some analytical tasks, and should be considered further.\n",
    "    '''\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    #Https and re-tweet.... remove them\n",
    "    stop_words = [\"https\",\"rt\"]\n",
    "    \n",
    "    sum_words = X.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items() if word != query_word and word.lower() not in stop_words]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)[:n_words]\n",
    "    word_freq_dict = {}\n",
    "    for word, freq in words_freq:\n",
    "        word_freq_dict[word] = freq\n",
    "    return word_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(review):\n",
    "    return TextBlob(review).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_users_and_sentiment(tweets):\n",
    "    top_tweepers = {}\n",
    "    sentiment_data = []\n",
    "    group_labels = []\n",
    "    for i, minutes_ago  in enumerate(time_steps):\n",
    "        time_delta = pd.to_datetime('today') - pd.Timedelta(120 + minutes_ago, 'minutes')\n",
    "        tmp_df = tweets[tweets[\"created_at\"] >= time_delta.tz_localize('UTC')]\n",
    "        top_tweepers[str(minutes_ago) + \" minutes ago\"] = Counter(tmp_df.author_id).most_common(10)  \n",
    "        sentiment = tmp_df.text.apply(find_sentiment).values\n",
    "        if(sentiment.shape[0]==0):\n",
    "            continue\n",
    "        sentiment_data.append(sentiment)\n",
    "        group_labels.append(str(minutes_ago) + \" minutes ago\" )\n",
    "#         sentiment_plots[str(minutes_ago) + \" minutes ago\"] = ff.create_distplot([sentiment.values],[\"distplot\"],  show_hist =False)\n",
    "\n",
    "    return top_tweepers, ff.create_distplot(sentiment_data, group_labels,  show_hist =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a plotly dashboard to visualize the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output,State\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from wordcloud import WordCloud\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:1111/\n"
     ]
    }
   ],
   "source": [
    "time_steps = [1,5,15]\n",
    "\n",
    "app.layout = html.Div(children=[\n",
    "    html.H1(children='Twitter statistics Dashboard'),\n",
    "\n",
    "    html.H3(children='''\n",
    "        Input a desired search term!\n",
    "    ''',style={\"textAlign\":\"center\"}),\n",
    "    html.Section([\n",
    "            html.Section([\n",
    "                html.Div([\n",
    "                    dcc.Input(id='search-term', value='kahoot', type='text'),\n",
    "                    html.Button('SEARCH!', id='search-button',n_clicks=0,style={\"backgroundCcolor\":\"white\"})\n",
    "                ],style={\n",
    "                \"width\":\"10%\",\n",
    "                \"display\":\"flex\",\n",
    "                \"margin\":\"auto\",\n",
    "                \"justifyContent\":\"space-around\"\n",
    "                })\n",
    "            ],style={\n",
    "                \"marginBottom\":\"2em\"\n",
    "            })\n",
    "    ]),\n",
    "    html.Section([\n",
    "        html.H2(children=\"Number of hits:\",style={\"textAlign\":\"center\",\"background\":\"white\"}),\n",
    "        html.Section([\n",
    "            html.H6(id=\"query-count\", style={\"background\":\"white\",\"margin\":\"auto\",\"textAlign\":\"left\",\"paddingBottom\":\"2em\"})\n",
    "        ]),\n",
    "        html.H2(children=\"Frequent search terms:\",style={\"textAlign\":\"center\",\"background\":\"white\"}),\n",
    "        html.Section([\n",
    "            html.H4(\"Last minute:\"),\n",
    "            html.H4(\"Last 5 minutes:\"),\n",
    "            html.H4(\"Last 15 minutes:\")\n",
    "        ], style= {\n",
    "            \"margin\":\"auto\",\n",
    "            \"display\":\"flex\",\n",
    "            \"width\":\"70%\",\n",
    "            \"justifyContent\":\"space-between\"\n",
    "        }),\n",
    "        html.Section([\n",
    "            html.Img(id=\"wc_1\",style={\"width\":\"20%\"}),\n",
    "            html.Img(id=\"wc_5\",style={\"width\":\"20%\"}),\n",
    "            html.Img(id=\"wc_15\",style={\"width\":\"20%\"}),\n",
    "        ],style={\n",
    "            \"marginTop\":\"2rem\",\n",
    "            \"display\":\"flex\",\n",
    "            \"margin\":\"auto\",\n",
    "            \"justifyContent\":\"space-between\",\n",
    "            \"width\":\"100%\"\n",
    "        }),\n",
    "        html.H2(children=\"Most active users:\",style={\"textAlign\":\"center\",\"background\":\"white\"}),\n",
    "        html.Section([\n",
    "            html.H6(id=\"top_tweepers\", style={\"background\":\"white\",\"margin\":\"auto\",\"textAlign\":\"left\",\"paddingBottom\":\"2em\"})\n",
    "        ]),\n",
    "        \n",
    "        html.H2(children=\"Sentiment for recent tweets:\",style={\"textAlign\":\"center\",\"background\":\"white\"}),\n",
    "\n",
    "        dcc.Graph(id=\"sentiment_dist\",style={\"width\":\"80%\",\"margin\":\"auto\"})\n",
    "\n",
    "        \n",
    "    ],style={\"background\":\"white\"})\n",
    "    \n",
    "#     html.Section([\n",
    "#         html.H6(id=\"query-count\", style={\"background\":\"white\",\"margin\":\"auto\",\"textAlign\":\"left\"})\n",
    "#     ]),\n",
    "    \n",
    "],\n",
    "style={\n",
    "    \"background\":\"radial-gradient(circle, rgba(63,94,251,0.5298494397759104) 0%, rgba(252,70,107,1) 100%)\"\n",
    "})\n",
    "\n",
    "def plot_wordcloud(frequent_terms):\n",
    "    wc = WordCloud(background_color='white', width=480, height=360)\n",
    "    wc.fit_words(frequent_terms)\n",
    "    return wc.to_image()\n",
    "\n",
    "@app.callback(\n",
    "    Output('query-count','children'),\n",
    "    [Input('search-button', 'n_clicks')],\n",
    "    state=[State(component_id='search-term', component_property='value')]\n",
    ")\n",
    "\n",
    "def queryCount(search_button_clicks,search_term):\n",
    "    query_count = get_query_count(search_term)\n",
    "    return (\n",
    "        html.Section([\n",
    "        \n",
    "            html.Div(f'last minute:\\t {query_count[\"1 minutes ago\"]}'),\n",
    "            html.Div(f'last 5 minutes:\\t {query_count[\"5 minutes ago\"]}'),\n",
    "            html.Div(f'last 15 minutes:\\t {query_count[\"15 minutes ago\"]}'),\n",
    "            html.Div(f'total:\\t {query_count[\"total\"]}')\n",
    "            ],\n",
    "            style ={\n",
    "            \"background\":\"white\",\n",
    "            \"width\":\"50%\",\n",
    "            \"display\":\"flex\",\n",
    "            \"margin\":\"auto\",\n",
    "            \"justifyContent\":\"space-around\",\n",
    "            \"flexDirection\":\"rows\"\n",
    "            })\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('wc_1', 'src'),\n",
    "    Output('wc_5', 'src'),\n",
    "    Output('wc_15', 'src'),\n",
    "    #Output('top_tweepers', 'children'),\n",
    "    #Output('sentiment_dist','figure'),\n",
    "    [Input('search-button', 'n_clicks')],\n",
    "    state=[State(component_id='search-term', component_property='value')]\n",
    ")\n",
    "\n",
    "def tweetFrequency(search_button_clicks,search_term):   \n",
    "    df_tweets = get_tweets(search_term)\n",
    "    frequent_terms = {}\n",
    "    for minutes_ago in time_steps:\n",
    "        time_delta = pd.to_datetime('today') - pd.Timedelta(120 + minutes_ago, 'minutes')\n",
    "        frequent_terms[str(minutes_ago) + \" minutes ago\"] = get_most_frequent_words(search_term,df_tweets[df_tweets[\"created_at\"] >= time_delta.tz_localize('UTC')].text.values, 10)  \n",
    "    \n",
    "    word_clouds = []\n",
    "    for terms in frequent_terms.values():  \n",
    "        img = BytesIO()\n",
    "        plot_wordcloud(terms).save(img, format='PNG')\n",
    "        word_clouds.append('data:image/png;base64,{}'.format(base64.b64encode(img.getvalue()).decode()))\n",
    "        \n",
    "        \n",
    "    return (\n",
    "        word_clouds[0],\n",
    "        word_clouds[1],\n",
    "        word_clouds[2]\n",
    "    )\n",
    "\n",
    "@app.callback(\n",
    "    Output('top_tweepers', 'children'),\n",
    "    Output('sentiment_dist','figure'),\n",
    "    [Input('search-button', 'n_clicks')],\n",
    "    state=[State(component_id='search-term', component_property='value')]\n",
    ")\n",
    "\n",
    "def topUsersAndSentiment(search_button_clicks,search_term):   \n",
    "    df_tweets = get_tweets(search_term)        \n",
    "    top_tweepers, sentiment_figure = get_top_users_and_sentiment(df_tweets)\n",
    "    sentiment_figure.update_layout(width=1500,height=800)\n",
    "    return (\n",
    "        f'''Last minute:\\t {str(top_tweepers[\"1 minutes ago\"])}\n",
    "        \\nLast 5 minutes:\\t {str(top_tweepers[\"5 minutes ago\"])}\n",
    "        \\nLast 15 minutes:\\t {str(top_tweepers[\"15 minutes ago\"])}''',\n",
    "        sentiment_figure\n",
    "    )\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, port=1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
